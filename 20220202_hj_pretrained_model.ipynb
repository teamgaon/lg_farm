{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20220202_hj_plantdoc.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMSBOdvT89icWErin2druBX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/teamgaon/lg_farm/blob/main/20220202_hj_pretrained_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jH9lxzd1J55l"
      },
      "outputs": [],
      "source": [
        "!pip install ttach"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install efficientnet_pytorch"
      ],
      "metadata": {
        "id": "PbZJIcAIKBSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n",
        "# k80 -> T4 -> P100"
      ],
      "metadata": {
        "id": "lQAdZwHeKCS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 사용 패키지"
      ],
      "metadata": {
        "id": "g_2U2zCJKDAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import ttach as tta\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensor\n",
        "from sklearn.model_selection import KFold\n",
        "import torch.nn as nn\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "KY1RN7bIKFan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ZMTOk7HwKIV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/LG/plant/train.zip"
      ],
      "metadata": {
        "id": "qsLIMTzDKJUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/LG/plant/test.zip"
      ],
      "metadata": {
        "id": "OmA6u-NxKKZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/LG/PlantDoc-Dataset-master.zip"
      ],
      "metadata": {
        "id": "5lJUZAlWKL2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## customDataset 제작"
      ],
      "metadata": {
        "id": "eVz3VFzoKM9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_classes = pd.read_table('/content/drive/MyDrive/LG/plant/train_classes.txt',names=['filename', 'Apple Scab Leaf', 'Apple leaf', 'Apple rust leaf', 'Bell_pepper leaf','Bell_pepper leaf spot', 'Blueberry leaf', 'Cherry leaf', 'Corn Gray leaf spot', 'Corn leaf blight', 'Corn rust leaf', 'Peach leaf', 'Potato leaf', 'Potato leaf early blight', 'Potato leaf late blight', 'Raspberry leaf', 'Soyabean leaf', 'Soybean leaf', 'Squash Powdery mildew leaf', 'Strawberry leaf', 'Tomato Early blight leaf', 'Tomato Septoria leaf spot', 'Tomato leaf', 'Tomato leaf bacterial spot', 'Tomato leaf late blight', 'Tomato leaf mosaic virus', 'Tomato leaf yellow virus', 'Tomato mold leaf', 'Tomato two spotted spider mites leaf', 'grape leaf', 'grape leaf black rot'],sep=',')\n",
        "test_classes = pd.read_table('/content/drive/MyDrive/LG/plant/test_classes.txt',names=['filename', 'Apple Scab Leaf', 'Apple leaf', 'Apple rust leaf', 'Bell_pepper leaf','Bell_pepper leaf spot', 'Blueberry leaf', 'Cherry leaf', 'Corn Gray leaf spot', 'Corn leaf blight', 'Corn rust leaf', 'Peach leaf', 'Potato leaf', 'Potato leaf early blight', 'Potato leaf late blight', 'Raspberry leaf', 'Soyabean leaf', 'Soybean leaf', 'Squash Powdery mildew leaf', 'Strawberry leaf', 'Tomato Early blight leaf', 'Tomato Septoria leaf spot', 'Tomato leaf', 'Tomato leaf bacterial spot', 'Tomato leaf late blight', 'Tomato leaf mosaic virus', 'Tomato leaf yellow virus', 'Tomato mold leaf', 'Tomato two spotted spider mites leaf', 'grape leaf', 'grape leaf black rot'],sep=',')"
      ],
      "metadata": {
        "id": "cO23pA00KQUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1이 들어있는 컬럼의 이름을 name에 저장"
      ],
      "metadata": {
        "id": "YPlU85VxKexh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def name_to_label(name:str):\n",
        "  for col in train_classes.columns:\n",
        "    if train_classes.loc[train_classes['filename'] == name, col].item() == 1:\n",
        "      return col\n",
        "  \n",
        "train_classes['name'] = train_classes['filename'].map(name_to_label)\n",
        "train_disease = train_classes[['filename','name']]"
      ],
      "metadata": {
        "id": "hEMih6WoKTYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def name_to_label(name:str):\n",
        "  for col in test_classes.columns:\n",
        "    if test_classes.loc[test_classes['filename'] == name, col].item() == 1:\n",
        "      return col\n",
        "\n",
        "test_classes['name'] = test_classes['filename'].map(name_to_label)\n",
        "\n",
        "test_disease = test_classes[['filename','name']]"
      ],
      "metadata": {
        "id": "hirIpTKhKVnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "disease dict 만들기"
      ],
      "metadata": {
        "id": "SzHu1OjQKn0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_name = train_disease['name'].unique()\n",
        "test_name = test_disease['name'].unique()\n",
        "sum_name = np.concatenate((train_name,test_name))"
      ],
      "metadata": {
        "id": "mJaOi8LqKr-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum_name = list(set(sum_name))\n",
        "\n",
        "# 빠져 있는 값 넣기\n",
        "sum_name.append('Tomato two spotted spider mites leaf')"
      ],
      "metadata": {
        "id": "Z4Tt2pm7Ks34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_dict={}\n",
        "for i in range(0,31):\n",
        "  my_dict[sum_name[i]] = i\n",
        "my_dict"
      ],
      "metadata": {
        "id": "tHbRj_ypK2pZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 불러오기\n",
        "plantdoc_train = sorted(glob('/content/PlantDoc-Dataset-master/train/*/*'))\n",
        "plantdoc_test = sorted(glob('/content/PlantDoc-Dataset-master/test/*/*'))"
      ],
      "metadata": {
        "id": "exm9eq1ZK-i5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, files, labels=None, mode='train',my_dict=None):\n",
        "        self.mode = mode\n",
        "        self.files = files\n",
        "        self.my_dict = my_dict\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        file = self.files[i]\n",
        "\n",
        "        # image\n",
        "        image_path = plantdoc_train[i]\n",
        "        img = cv2.imread(image_path)\n",
        "        img = cv2.resize(img, dsize=(528, 528), interpolation=cv2.INTER_AREA)\n",
        "        img = img.astype(np.float32)/255\n",
        "        img = np.transpose(img, (2,0,1))\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            # json_path = f'{file}/{file_name}.json'\n",
        "            # with open(json_path, 'r') as f:\n",
        "            #     json_file = json.load(f)\n",
        "            \n",
        "            # crop = json_file['annotations']['crop']\n",
        "            # disease = json_file['annotations']['disease']\n",
        "            # risk = json_file['annotations']['risk']\n",
        "            \n",
        "            self.labels = image_path.split('/')[-2]\n",
        "            self.label = self.my_dict[self.labels]\n",
        "\n",
        "            # augmentation = random.randint(0,2)\n",
        "            # if augmentation==1:\n",
        "            #     img = img[::-1].copy()\n",
        "            # elif augmentation==2:\n",
        "            #     img = img[:,::-1].copy()\n",
        "            # img = transforms.ToTensor()(img)\n",
        "            \n",
        "            return {\n",
        "                'img' : torch.tensor(img, dtype=torch.float32),\n",
        "                # 'csv_feature' : torch.tensor(csv_feature, dtype=torch.float32),\n",
        "                'label' : torch.tensor(self.label, dtype=torch.long)\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                'img' : torch.tensor(img, dtype=torch.float32),\n",
        "                # 'csv_feature' : torch.tensor(csv_feature, dtype=torch.float32)\n",
        "                'label' : torch.tensor(self.label, dtype=torch.long)\n",
        "            }"
      ],
      "metadata": {
        "id": "I2Hy1N2wLEzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 하이퍼파라미터 및 변수"
      ],
      "metadata": {
        "id": "1tU7PCcpLLKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\")\n",
        "batch_size = 16\n",
        "class_n = len(my_dict)\n",
        "learning_rate = 1e-4\n",
        "embedding_dim = 512\n",
        "max_len = 24*6\n",
        "dropout_rate = 0.001\n",
        "epochs = 15\n",
        "k_folds = 5\n",
        "vision_pretrain = True\n",
        "save_path = '/content/drive/MyDrive/LG'"
      ],
      "metadata": {
        "id": "4VFX2_93LNty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터셋 구성"
      ],
      "metadata": {
        "id": "FwMoeVVLLQkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms = A.Compose([\n",
        "A.Normalize(),\n",
        "ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "mCrxzfAJLVpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(plantdoc_train,my_dict = my_dict)\n",
        "# val_dataset = CustomDataset(val)\n",
        "test_dataset = CustomDataset(plantdoc_test,my_dict = my_dict)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(plantdoc_train, batch_size=batch_size, num_workers=2, shuffle=True)\n",
        "# val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, num_workers=2, shuffle=False)\n",
        "test_dataloader = torch.utils.data.DataLoader(plantdoc_test, batch_size=batch_size, num_workers=2, shuffle=False)"
      ],
      "metadata": {
        "id": "Z-K8zPbJLWoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델\n",
        "\n",
        "이미지 분류 모델 : effientnet_b6"
      ],
      "metadata": {
        "id": "5ltac1CuLuyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_Encoder(nn.Module):\n",
        "    def __init__(self, class_n, rate=0.1):\n",
        "        super(CNN_Encoder, self).__init__()\n",
        "        self.model = models.efficientnet_b2(pretrained=True)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        output = self.model(inputs)\n",
        "        return output"
      ],
      "metadata": {
        "id": "ioskhZ82LiLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "앙상블"
      ],
      "metadata": {
        "id": "q0m7Prw6L5vw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN2RNN(nn.Module):\n",
        "    def __init__(self, max_len, embedding_dim, class_n, rate):\n",
        "        super(CNN2RNN, self).__init__()\n",
        "        self.cnn = CNN_Encoder(embedding_dim, rate)\n",
        "        self.rnn = RNN_Decoder(max_len, embedding_dim, class_n, rate)\n",
        "        \n",
        "    def forward(self, img, seq):\n",
        "        cnn_output = self.cnn(img)\n",
        "        output = self.rnn(cnn_output, seq)\n",
        "        \n",
        "        return output"
      ],
      "metadata": {
        "id": "9hum0EfaL5Np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.efficientnet_b6(pretrained=True)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "XyaMbuCwMJZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 학습"
      ],
      "metadata": {
        "id": "aQ620FNKMK5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer=optimizer,\n",
        "                                        lr_lambda=lambda epoch: 0.95 ** epoch,\n",
        "                                        last_epoch=-1,\n",
        "                                        verbose=False)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "ZOfojonSMLlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_function(real, pred):    \n",
        "    real = real.cpu()\n",
        "    pred = torch.argmax(pred, dim=1).cpu()\n",
        "    score = f1_score(real, pred, average='macro')\n",
        "    return score\n",
        "\n",
        "def train_step(batch_item, training):\n",
        "    img = batch_item['img'].to(device)\n",
        "    label = batch_item['label'].to(device)\n",
        "    if training is True:\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        with torch.cuda.amp.autocast():\n",
        "            output = model(img)\n",
        "            loss = criterion(output, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        score = accuracy_function(label, output)\n",
        "        return loss, score\n",
        "    else:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            output = model(img)\n",
        "            loss = criterion(output, label)\n",
        "        score = accuracy_function(label, output)\n",
        "        return loss, score"
      ],
      "metadata": {
        "id": "k-G_IvhyMM3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torchvision.transforms as transforms\n",
        "import gc"
      ],
      "metadata": {
        "id": "tGHiw727MOOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "metadata": {
        "id": "hT7LVtiWMPIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_plot, val_loss_plot = [], []\n",
        "metric_plot, val_metric_plot = [], []\n",
        "      \n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
        "  \n",
        "# Start print\n",
        "print('--------------------------------')\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "for fold, (train_ids, test_ids) in enumerate(kfold.split(train_dataset)):\n",
        "  gc.collect()\n",
        "  # loss_plot, val_loss_plot = [], []\n",
        "  # metric_plot, val_metric_plot = [], []\n",
        "  \n",
        "  # Print\n",
        "  print('')\n",
        "  print(f'FOLD {fold}')\n",
        "  print('--------------------------------')\n",
        "  \n",
        "  # Sample elements randomly from a given list of ids, no replacement.\n",
        "  train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "  test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
        "  \n",
        "  # Define data loaders for training and testing data in this fold\n",
        "  train_dataloader = torch.utils.data.DataLoader(\n",
        "                    train_dataset, \n",
        "                    batch_size=batch_size, sampler=train_subsampler, num_workers=2)\n",
        "  val_dataloader = torch.utils.data.DataLoader(\n",
        "                    train_dataset,\n",
        "                    batch_size=batch_size, sampler=test_subsampler, num_workers=2)\n",
        "  \n",
        "  model = models.efficientnet_b6(max_len=max_len, embedding_dim=embedding_dim, class_n=class_n, rate=dropout_rate)\n",
        "  model = model.to(device)\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer=optimizer,\n",
        "                                          lr_lambda=lambda epoch: 0.95 ** epoch,\n",
        "                                          last_epoch=-1,\n",
        "                                          verbose=False)\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  \n",
        "  for epoch in range(epochs):\n",
        "      total_loss, total_val_loss = 0, 0\n",
        "      total_acc, total_val_acc = 0, 0\n",
        "      \n",
        "      tqdm_dataset = tqdm(enumerate(train_dataloader))\n",
        "      training = True\n",
        "      for batch, batch_item in tqdm_dataset:\n",
        "          batch_loss, batch_acc = train_step(batch_item, training)\n",
        "          total_loss += batch_loss\n",
        "          total_acc += batch_acc\n",
        "          \n",
        "          tqdm_dataset.set_postfix({\n",
        "              'Epoch': epoch + 1,\n",
        "              'Loss': '{:06f}'.format(batch_loss.item()),\n",
        "              'Mean Loss' : '{:06f}'.format(total_loss/(batch+1)),\n",
        "              'Mean F-1' : '{:06f}'.format(total_acc/(batch+1))\n",
        "          })\n",
        "      loss_plot.append(total_loss/(batch+1))\n",
        "      metric_plot.append(total_acc/(batch+1))\n",
        "      \n",
        "      tqdm_dataset = tqdm(enumerate(val_dataloader))\n",
        "      training = False\n",
        "      for batch, batch_item in tqdm_dataset:\n",
        "          batch_loss, batch_acc = train_step(batch_item, training)\n",
        "          total_val_loss += batch_loss\n",
        "          total_val_acc += batch_acc\n",
        "          \n",
        "          tqdm_dataset.set_postfix({\n",
        "              'Epoch': epoch + 1,\n",
        "              'Val Loss': '{:06f}'.format(batch_loss.item()),\n",
        "              'Mean Val Loss' : '{:06f}'.format(total_val_loss/(batch+1)),\n",
        "              'Mean Val F-1' : '{:06f}'.format(total_val_acc/(batch+1))\n",
        "          })\n",
        "      val_loss_plot.append(total_val_loss/(batch+1))\n",
        "      val_metric_plot.append(total_val_acc/(batch+1))\n",
        "      scheduler.step()\n",
        "\n",
        "      if np.max(val_metric_plot) == val_metric_plot[-1]:\n",
        "          torch.save(model.state_dict(), save_path+'pretrained_model.pt')\n",
        "          print('best')"
      ],
      "metadata": {
        "id": "BEe1AJdaMQJx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}